{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1 — Hybrid Data Factory\n",
        "\n",
        "**Strategy D: Hybrid Emitter + nvcc Validation**\n",
        "\n",
        "| Source | Speed | Purpose |\n",
        "|--------|-------|---------|\n",
        "| **Pure-Python PTX Emitter** (90%) | ~50k pairs/min | Bulk training data — no GPU/nvcc needed |\n",
        "| **nvcc compilation** (10%) | ~5 pairs/sec | Real compiler patterns + validation |\n",
        "\n",
        "The emitter generates deterministic PTX instruction sequences from AST nodes\n",
        "using a mini-compiler approach (register allocation, load/store generation,\n",
        "expression codegen). Output goes through the same `normalize_ptx()` pipeline\n",
        "as real nvcc output, so the model sees a unified token format.\n",
        "\n",
        "**Runtime**: CPU is enough. No GPU needed for this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Run this cell first on Google Colab to clone the repo ---\n",
        "import os\n",
        "if os.path.exists(\"/content\"):\n",
        "    %cd /content\n",
        "    !rm -rf /content/DeepPTX\n",
        "    !git clone https://github.com/ns-1456/DeepPTX.git /content/DeepPTX\n",
        "    %cd /content/DeepPTX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not create leading directories of '/content/DeepPTX': Read-only file system\n",
            "[Errno 2] No such file or directory: '/content/DeepPTX'\n",
            "/Users/ns/Projects for Resume/Neural PTX Decompiler/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q pyarrow tqdm\n",
        "\n",
        "# Optional: mount Google Drive for persistent storage\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "# OUTPUT_DIR = \"/content/drive/MyDrive/NeuralPTX\"\n",
        "\n",
        "OUTPUT_DIR = \".\"  # saves to repo root; uncomment above for Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ======================= Configuration =======================\n",
        "TARGET_PAIRS   = 100_000   # total pairs to generate\n",
        "EMITTER_FRAC   = 0.90      # 90% from pure-Python emitter\n",
        "NVCC_FRAC      = 0.10      # 10% from real nvcc (set to 0 if no nvcc)\n",
        "NVCC_BATCH     = 200       # .cu files per nvcc round\n",
        "# =============================================================\n",
        "\n",
        "EMITTER_TARGET = int(TARGET_PAIRS * EMITTER_FRAC)\n",
        "NVCC_TARGET    = TARGET_PAIRS - EMITTER_TARGET\n",
        "\n",
        "print(f\"Target: {TARGET_PAIRS:,} total\")\n",
        "print(f\"  Emitter: {EMITTER_TARGET:,} pairs (instant)\")\n",
        "print(f\"  nvcc:    {NVCC_TARGET:,} pairs (slower, real compiler)\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target: 100,000 total\n",
            "  Emitter: 90,000 pairs (instant)\n",
            "  nvcc:    10,000 pairs (slower, real compiler)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, os, random, time\n",
        "\n",
        "REPO_ROOT = \"/content/DeepPTX\" if os.path.exists(\"/content/DeepPTX\") else os.path.abspath(\"..\")\n",
        "if REPO_ROOT not in sys.path:\n",
        "    sys.path.insert(0, REPO_ROOT)\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from ptx_decompiler.data import (\n",
        "    parse_sexp,\n",
        "    ast_to_cuda,\n",
        "    normalize_ptx,\n",
        "    PTXEmitter,\n",
        ")\n",
        "from ptx_decompiler.data.grammar import TIER_CLASSES, sample_tier\n",
        "\n",
        "print(\"Imports OK\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imports OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part A — Pure-Python Emitter (90% of data)\n",
        "\n",
        "The `PTXEmitter` translates AST nodes directly into PTX instructions\n",
        "without calling `nvcc`. This is a deterministic mini-compiler that:\n",
        "\n",
        "1. Emits thread-index prologue (`mov.u32 %r0, %ctaid.x; ...`)\n",
        "2. Loads array variables via `ld.param.u64` + `ld.global.f32`\n",
        "3. Recursively compiles expression nodes to PTX arithmetic\n",
        "4. Stores result via `st.global.f32`\n",
        "5. Normalizes output through the same pipeline as nvcc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "random.seed(42)\n",
        "emitter = PTXEmitter()\n",
        "\n",
        "emitter_data = []\n",
        "emitter_fails = 0\n",
        "\n",
        "pbar = tqdm(total=EMITTER_TARGET, desc=\"Emitter\", unit=\"pair\")\n",
        "\n",
        "while len(emitter_data) < EMITTER_TARGET:\n",
        "    tier_id, gen = sample_tier()\n",
        "    ast_node = gen.generate()\n",
        "    ast_sexp = ast_node.to_sexp()\n",
        "    cuda_source = ast_to_cuda(ast_sexp)\n",
        "\n",
        "    try:\n",
        "        ptx_norm = emitter.emit_normalized(ast_node)\n",
        "        if not ptx_norm.strip():\n",
        "            emitter_fails += 1\n",
        "            continue\n",
        "        emitter_data.append({\n",
        "            \"ptx_normalized\": ptx_norm,\n",
        "            \"ast_sexp\": ast_sexp,\n",
        "            \"cuda_source\": cuda_source,\n",
        "            \"tier\": tier_id,\n",
        "            \"complexity_score\": gen.complexity_score,\n",
        "            \"source\": \"emitter\",\n",
        "        })\n",
        "        pbar.update(1)\n",
        "    except Exception as e:\n",
        "        emitter_fails += 1\n",
        "\n",
        "pbar.close()\n",
        "print(f\"\\nEmitter: {len(emitter_data):,} pairs | {emitter_fails} failures\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d970cf2719c7446d85773b6f681b35e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Emitter:   0%|          | 0/90000 [00:00<?, ?pair/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Emitter: 90,000 pairs | 0 failures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B — nvcc Compilation (10% of data)\n",
        "\n",
        "For real compiler diversity: generate CUDA, compile with `nvcc -ptx -O0`,\n",
        "normalize the output. If nvcc is not available (e.g., on a Mac), this section\n",
        "is skipped and the emitter provides 100% of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, tempfile, shutil\n",
        "\n",
        "# ---- Detect nvcc ----\n",
        "HAS_NVCC = shutil.which(\"nvcc\") is not None\n",
        "\n",
        "if HAS_NVCC:\n",
        "    WORK_DIR = tempfile.mkdtemp(prefix=\"ptx_batch_\")\n",
        "    NUM_PARALLEL = max(os.cpu_count() or 2, 2) * 3\n",
        "    \n",
        "    # Sanity check\n",
        "    test_cu = Path(WORK_DIR) / \"test.cu\"\n",
        "    test_ptx = Path(WORK_DIR) / \"test.ptx\"\n",
        "    test_cu.write_text('extern \"C\" __global__ void k(float* a) { a[0] = 1.0f; }')\n",
        "    r = subprocess.run([\"nvcc\", \"-ptx\", \"-O0\", str(test_cu), \"-o\", str(test_ptx)],\n",
        "                       capture_output=True, text=True, timeout=30)\n",
        "    if r.returncode != 0:\n",
        "        print(f\"WARNING: nvcc sanity check failed — falling back to emitter-only\")\n",
        "        print(f\"stderr: {r.stderr[:200]}\")\n",
        "        HAS_NVCC = False\n",
        "    else:\n",
        "        print(f\"nvcc OK ({test_ptx.stat().st_size} bytes)\")\n",
        "    test_cu.unlink(missing_ok=True)\n",
        "    test_ptx.unlink(missing_ok=True)\n",
        "    \n",
        "    # Detect arch\n",
        "    NVCC_ARCH = []\n",
        "    if HAS_NVCC:\n",
        "        test_cu.write_text('extern \"C\" __global__ void k(float* a) { a[0] = 1.0f; }')\n",
        "        r = subprocess.run([\"nvcc\", \"-ptx\", \"-O0\", \"-arch=sm_75\", str(test_cu), \"-o\", str(test_ptx)],\n",
        "                           capture_output=True, text=True, timeout=30)\n",
        "        if r.returncode == 0:\n",
        "            NVCC_ARCH = [\"-arch=sm_75\"]\n",
        "            print(\"Using -arch=sm_75 (T4)\")\n",
        "        else:\n",
        "            print(\"Using default arch\")\n",
        "        test_cu.unlink(missing_ok=True)\n",
        "        test_ptx.unlink(missing_ok=True)\n",
        "else:\n",
        "    print(\"nvcc not found — using emitter-only mode (100% synthetic PTX)\")\n",
        "    print(\"This is fine! The emitter produces realistic, training-ready PTX.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc not found — using emitter-only mode (100% synthetic PTX)\n",
            "This is fine! The emitter produces realistic, training-ready PTX.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nvcc_data = []\n",
        "\n",
        "if HAS_NVCC and NVCC_TARGET > 0:\n",
        "    nvcc_fails = 0\n",
        "\n",
        "    def compile_batch_parallel(batch, work_dir, max_concurrent):\n",
        "        \"\"\"Write .cu files, fire parallel nvcc processes, read .ptx results.\"\"\"\n",
        "        n = len(batch)\n",
        "        cu_paths, ptx_paths = [], []\n",
        "        for i, (_, cuda_src, _, _) in enumerate(batch):\n",
        "            cu = Path(work_dir) / f\"{i}.cu\"\n",
        "            ptx = Path(work_dir) / f\"{i}.ptx\"\n",
        "            cu.write_text(cuda_src, encoding=\"utf-8\")\n",
        "            cu_paths.append(cu)\n",
        "            ptx_paths.append(ptx)\n",
        "\n",
        "        # Fire in waves\n",
        "        for start in range(0, n, max_concurrent):\n",
        "            end = min(start + max_concurrent, n)\n",
        "            procs = []\n",
        "            for i in range(start, end):\n",
        "                cmd = [\"nvcc\", \"-ptx\", \"-O0\"] + NVCC_ARCH + [str(cu_paths[i]), \"-o\", str(ptx_paths[i])]\n",
        "                p = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                procs.append((i, p))\n",
        "            for i, p in procs:\n",
        "                try:\n",
        "                    p.wait(timeout=30)\n",
        "                except subprocess.TimeoutExpired:\n",
        "                    p.kill()\n",
        "\n",
        "        results = []\n",
        "        for i, (ast_sexp, cuda_src, tier_id, score) in enumerate(batch):\n",
        "            if ptx_paths[i].exists() and ptx_paths[i].stat().st_size > 0:\n",
        "                ptx_raw = ptx_paths[i].read_text(encoding=\"utf-8\")\n",
        "                ptx_norm = normalize_ptx(ptx_raw)\n",
        "                if ptx_norm.strip():\n",
        "                    results.append({\n",
        "                        \"ptx_normalized\": ptx_norm,\n",
        "                        \"ast_sexp\": ast_sexp,\n",
        "                        \"cuda_source\": cuda_src,\n",
        "                        \"tier\": tier_id,\n",
        "                        \"complexity_score\": score,\n",
        "                        \"source\": \"nvcc\",\n",
        "                    })\n",
        "            cu_paths[i].unlink(missing_ok=True)\n",
        "            ptx_paths[i].unlink(missing_ok=True)\n",
        "        return results\n",
        "\n",
        "    pbar = tqdm(total=NVCC_TARGET, desc=\"nvcc\", unit=\"pair\")\n",
        "\n",
        "    while len(nvcc_data) < NVCC_TARGET:\n",
        "        need = min(NVCC_BATCH, int((NVCC_TARGET - len(nvcc_data)) * 1.15) + 10)\n",
        "        batch = []\n",
        "        for _ in range(need):\n",
        "            tier_id, gen = sample_tier()\n",
        "            ast = gen.generate()\n",
        "            ast_sexp = ast.to_sexp()\n",
        "            cuda_source = ast_to_cuda(ast_sexp)\n",
        "            batch.append((ast_sexp, cuda_source, tier_id, gen.complexity_score))\n",
        "\n",
        "        results = compile_batch_parallel(batch, WORK_DIR, NUM_PARALLEL)\n",
        "        nvcc_fails += len(batch) - len(results)\n",
        "\n",
        "        for row in results:\n",
        "            if len(nvcc_data) >= NVCC_TARGET:\n",
        "                break\n",
        "            nvcc_data.append(row)\n",
        "        pbar.n = len(nvcc_data)\n",
        "        pbar.refresh()\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"\\nnvcc: {len(nvcc_data):,} pairs | {nvcc_fails} failures\")\n",
        "else:\n",
        "    print(\"Skipping nvcc — emitter provides all data.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping nvcc — emitter provides all data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge & Shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = emitter_data + nvcc_data\n",
        "random.shuffle(data)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"\\nTotal dataset: {len(df):,} pairs\")\n",
        "print(f\"\\nSource breakdown:\")\n",
        "print(df[\"source\"].value_counts())\n",
        "print(f\"\\nTier distribution:\")\n",
        "print(df[\"tier\"].value_counts().sort_index())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total dataset: 90,000 pairs\n",
            "\n",
            "Source breakdown:\n",
            "source\n",
            "emitter    90000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Tier distribution:\n",
            "tier\n",
            "1    22307\n",
            "2    22516\n",
            "3    18105\n",
            "4    13544\n",
            "5     6178\n",
            "6     4615\n",
            "7     2735\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save to Parquet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "out_path = Path(OUTPUT_DIR) / \"dataset_100k.parquet\"\n",
        "df.to_parquet(out_path, index=False)\n",
        "print(f\"Saved to {out_path} ({out_path.stat().st_size / 1e6:.1f} MB)\")\n",
        "df.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved to dataset_100k.parquet (7.8 MB)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ptx_normalized</th>\n",
              "      <th>ast_sexp</th>\n",
              "      <th>cuda_source</th>\n",
              "      <th>tier</th>\n",
              "      <th>complexity_score</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u3...</td>\n",
              "      <td>(SIN (EXP A))</td>\n",
              "      <td>extern \"C\" __global__ void k(float* A, float* ...</td>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>emitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u3...</td>\n",
              "      <td>(ADD Y X)</td>\n",
              "      <td>extern \"C\" __global__ void k(float* A, float* ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>emitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u3...</td>\n",
              "      <td>(MIN Y X)</td>\n",
              "      <td>extern \"C\" __global__ void k(float* A, float* ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>emitter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      ptx_normalized       ast_sexp  \\\n",
              "0  mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u3...  (SIN (EXP A))   \n",
              "1  mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u3...      (ADD Y X)   \n",
              "2  mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u3...      (MIN Y X)   \n",
              "\n",
              "                                         cuda_source  tier  complexity_score  \\\n",
              "0  extern \"C\" __global__ void k(float* A, float* ...     5               5.0   \n",
              "1  extern \"C\" __global__ void k(float* A, float* ...     1               1.0   \n",
              "2  extern \"C\" __global__ void k(float* A, float* ...     1               1.0   \n",
              "\n",
              "    source  \n",
              "0  emitter  \n",
              "1  emitter  \n",
              "2  emitter  "
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n",
        "\n",
        "1. **Round-trip check**: AST → CUDA → (parse back) should match\n",
        "2. **Emitter sanity**: Spot-check that emitter PTX is non-empty and tokenizes well\n",
        "3. **Cross-check** (if nvcc available): Compare emitter vs nvcc instruction counts on same AST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from ptx_decompiler.data.renderer import CUDARenderer\n",
        "\n",
        "# --- Round-trip test ---\n",
        "n_check = min(500, len(df))\n",
        "rt_ok = 0\n",
        "for idx in tqdm(range(n_check), desc=\"Round-trip\", unit=\"pair\"):\n",
        "    row = df.iloc[idx]\n",
        "    tree = parse_sexp(row[\"ast_sexp\"])\n",
        "    rendered = CUDARenderer().kernel_source(tree)\n",
        "    if row[\"cuda_source\"].strip() == rendered.strip():\n",
        "        rt_ok += 1\n",
        "print(f\"\\nRound-trip: {rt_ok}/{n_check} OK ({rt_ok/n_check*100:.1f}%)\")\n",
        "\n",
        "# --- PTX sanity ---\n",
        "ptx_lengths = df[\"ptx_normalized\"].str.split().str.len()\n",
        "print(f\"\\nPTX token counts: min={ptx_lengths.min()}, median={ptx_lengths.median():.0f}, \"\n",
        "      f\"max={ptx_lengths.max()}, mean={ptx_lengths.mean():.1f}\")\n",
        "print(f\"Empty PTX rows: {(ptx_lengths == 0).sum()}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70e53389566b49a0b1bfaa12763527aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Round-trip:   0%|          | 0/500 [00:00<?, ?pair/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Round-trip: 500/500 OK (100.0%)\n",
            "\n",
            "PTX token counts: min=83, median=113, max=461, mean=114.1\n",
            "Empty PTX rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cross-check: emitter vs nvcc on same ASTs (if nvcc available) ---\n",
        "if HAS_NVCC:\n",
        "    from ptx_decompiler.data import compile_cuda_to_ptx_silent\n",
        "    \n",
        "    N_CROSS = 50\n",
        "    sample_rows = df[df[\"source\"] == \"emitter\"].sample(min(N_CROSS, len(df)), random_state=42)\n",
        "    match_count = 0\n",
        "    similar_count = 0\n",
        "    \n",
        "    emitter_check = PTXEmitter()\n",
        "    for _, row in tqdm(sample_rows.iterrows(), total=len(sample_rows), desc=\"Cross-check\"):\n",
        "        tree = parse_sexp(row[\"ast_sexp\"])\n",
        "        cuda_src = row[\"cuda_source\"]\n",
        "        \n",
        "        # Emitter PTX\n",
        "        emu_ptx = emitter_check.emit_normalized(tree)\n",
        "        \n",
        "        # nvcc PTX\n",
        "        ptx_raw = compile_cuda_to_ptx_silent(cuda_src, opt_level=\"-O0\")\n",
        "        if ptx_raw is None:\n",
        "            continue\n",
        "        nvcc_ptx = normalize_ptx(ptx_raw)\n",
        "        \n",
        "        # Compare\n",
        "        emu_tokens = len(emu_ptx.split())\n",
        "        nvcc_tokens = len(nvcc_ptx.split())\n",
        "        \n",
        "        if emu_ptx == nvcc_ptx:\n",
        "            match_count += 1\n",
        "            similar_count += 1\n",
        "        elif abs(emu_tokens - nvcc_tokens) / max(emu_tokens, nvcc_tokens, 1) < 0.3:\n",
        "            similar_count += 1\n",
        "    \n",
        "    print(f\"\\nCross-check ({len(sample_rows)} samples):\")\n",
        "    print(f\"  Exact match: {match_count} ({match_count/len(sample_rows)*100:.1f}%)\")\n",
        "    print(f\"  Similar (±30% tokens): {similar_count} ({similar_count/len(sample_rows)*100:.1f}%)\")\n",
        "    print(f\"  Note: Differences are expected — emitter uses deterministic codegen\")\n",
        "    print(f\"  while nvcc may reorder instructions or use different register allocation.\")\n",
        "else:\n",
        "    print(\"nvcc not available — skipping cross-check.\")\n",
        "    print(\"The emitter produces self-consistent PTX that the model can learn from.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc not available — skipping cross-check.\n",
            "The emitter produces self-consistent PTX that the model can learn from.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Quick look at a sample ---\n",
        "print(\"=\" * 60)\n",
        "row = df.iloc[0]\n",
        "print(f\"AST:  {row['ast_sexp']}\")\n",
        "print(f\"Tier: {row['tier']} | Source: {row['source']}\")\n",
        "print(f\"\\nCUDA:\")\n",
        "print(row['cuda_source'])\n",
        "print(f\"\\nNormalized PTX ({len(row['ptx_normalized'].split())} tokens):\")\n",
        "print(row['ptx_normalized'][:500])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "AST:  (SIN (EXP A))\n",
            "Tier: 5 | Source: emitter\n",
            "\n",
            "CUDA:\n",
            "extern \"C\" __global__ void k(float* A, float* B, float* C, float* X, float* Y, float* O, int N) {\n",
            "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "    if (i < N) {\n",
            "        O[i] = sinf(expf(A[i]));\n",
            "    }\n",
            "}\n",
            "\n",
            "Normalized PTX (91 tokens):\n",
            "mov.u32 %r0 , %s0.x mov.u32 %r1 , %s1.x mov.u32 %r2 , %s2.x mad.lo.s32 %r3 , %r0 , %r1 , %r2 ld.param.u32 %r4 , [k_param_6] setp.ge.s32 %p0 , %r3 , %r4 @%p0 bra EXIT ld.param.u64 %rd0 , [k_param_0] cvta.to.global.u64 %rd0 , %rd0 mul.wide.s32 %rd1 , %r3 , 4 add.s64 %rd1 , %rd0 , %rd1 ld.global.f32 %f0 , [%rd1] ex2.approx.f32 %f1 , %f0 sin.approx.f32 %f2 , %f1 ld.param.u64 %rd2 , [k_param_5] cvta.to.global.u64 %rd2 , %rd2 mul.wide.s32 %rd3 , %r3 , 4 add.s64 %rd3 , %rd2 , %rd3 st.global.f32 [%rd3] \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}