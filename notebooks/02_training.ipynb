{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2–3: Train DeepPTX\n",
        "\n",
        "Load Parquet dataset, build tokenizers, create DataLoaders with curriculum sampling,\n",
        "and train the Pointer-Generator Transformer.\n",
        "\n",
        "**Works on:**\n",
        "- Google Colab (T4/A100) — fastest, uses CUDA AMP\n",
        "- Mac M1/M2/M3 — uses MPS acceleration (no AMP, still fast)\n",
        "- CPU — slowest fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Run this cell first on Google Colab to clone the repo ---\n",
        "import os\n",
        "if os.path.exists(\"/content\"):\n",
        "    %cd /content\n",
        "    !rm -rf /content/DeepPTX\n",
        "    !git clone https://github.com/ns-1456/DeepPTX.git /content/DeepPTX\n",
        "    %cd /content/DeepPTX\n",
        "    !pip install -q torch pyarrow pandas wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = os.path.exists(\"/content\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    REPO_ROOT = \"/content/DeepPTX\"\n",
        "else:\n",
        "    REPO_ROOT = os.path.abspath(\"..\")\n",
        "\n",
        "if REPO_ROOT not in sys.path:\n",
        "    sys.path.insert(0, REPO_ROOT)\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from ptx_decompiler.utils import get_device, supports_amp, is_colab\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"AMP supported: {supports_amp(DEVICE)}\")\n",
        "if DEVICE.type == \"mps\":\n",
        "    print(\"Apple Silicon detected — using MPS acceleration\")\n",
        "    print(\"Tip: MPS is ~3-5x faster than CPU for training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================= Paths =======================\n",
        "# Colab: data on Drive or in /content\n",
        "# Local Mac: data in repo root (generated by notebook 01)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Option 1: Google Drive (persistent)\n",
        "    # from google.colab import drive; drive.mount(\"/content/drive\")\n",
        "    # DATA_PATH = \"/content/drive/MyDrive/NeuralPTX/dataset_100k.parquet\"\n",
        "    # SAVE_DIR  = Path(\"/content/drive/MyDrive/NeuralPTX/checkpoints\")\n",
        "\n",
        "    # Option 2: Local to Colab (faster I/O, lost on disconnect)\n",
        "    DATA_PATH = os.path.join(REPO_ROOT, \"dataset_100k.parquet\")\n",
        "    SAVE_DIR  = Path(REPO_ROOT) / \"checkpoints\"\n",
        "else:\n",
        "    # Local Mac / Linux\n",
        "    DATA_PATH = os.path.join(REPO_ROOT, \"dataset_100k.parquet\")\n",
        "    SAVE_DIR  = Path(REPO_ROOT) / \"checkpoints\"\n",
        "\n",
        "print(f\"Data: {DATA_PATH}\")\n",
        "print(f\"Checkpoints: {SAVE_DIR}\")\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH} — run notebook 01 first!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On Colab: apply AMP fix to copy_mechanism (scatter in float32, then cast back)\n",
        "if IN_COLAB:\n",
        "    import importlib\n",
        "    copy_mechanism_path = os.path.join(REPO_ROOT, \"ptx_decompiler\", \"model\", \"copy_mechanism.py\")\n",
        "    with open(copy_mechanism_path) as f:\n",
        "        content = f.read()\n",
        "    new_block = '''        contribution = contribution * valid.to(contribution.dtype)\n",
        "        # Scatter in float32 to avoid AMP dtype mismatches; use explicit dtype/device\n",
        "        dtype_orig = logits_vocab.dtype\n",
        "        logits_f32 = logits_vocab.to(torch.float32)\n",
        "        contribution_f32 = contribution.to(device=logits_f32.device, dtype=torch.float32)\n",
        "        logits_f32.scatter_add_(2, enc_ids_clamped, contribution_f32)\n",
        "        logits_vocab = logits_f32.to(dtype_orig)\n",
        "        return logits_vocab, p_gen'''\n",
        "    old1 = \"        contribution = (contribution * valid.float()).to(logits_vocab.dtype)\\n        logits_vocab.scatter_add_(2, enc_ids_clamped, contribution)\\n        return logits_vocab, p_gen\"\n",
        "    old2 = \"        contribution = contribution * valid.to(contribution.dtype)\\n        contribution = contribution.to(logits_vocab.dtype)\\n        logits_vocab.scatter_add_(2, enc_ids_clamped, contribution)\\n        return logits_vocab, p_gen\"\n",
        "    old3 = \"        contribution = contribution * valid.to(contribution.dtype)\\n        # Scatter in float32 to avoid AMP dtype mismatches with scatter_add_, then cast back\\n        dtype_orig = logits_vocab.dtype\\n        logits_vocab = logits_vocab.float().scatter_add_(\\n            2, enc_ids_clamped, contribution.float()\\n        ).to(dtype_orig)\\n        return logits_vocab, p_gen\"\n",
        "    applied = False\n",
        "    for old in (old1, old2, old3):\n",
        "        if old in content:\n",
        "            content = content.replace(old, new_block)\n",
        "            with open(copy_mechanism_path, \"w\") as f:\n",
        "                f.write(content)\n",
        "            print(\"Applied copy_mechanism AMP fix (float32 scatter).\")\n",
        "            applied = True\n",
        "            break\n",
        "    if not applied and \"contribution_f32\" in content:\n",
        "        print(\"copy_mechanism already patched.\")\n",
        "    elif not applied:\n",
        "        print(\"copy_mechanism.py format changed — check manually.\")\n",
        "    if \"ptx_decompiler.model.copy_mechanism\" in sys.modules:\n",
        "        import ptx_decompiler.model.copy_mechanism as _cm\n",
        "        importlib.reload(_cm)\n",
        "        print(\"Reloaded copy_mechanism module.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data & Build Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ptx_decompiler.data.dataset import load_parquet_for_training, collate_pad_batch, CurriculumSampler, PTXASTDataset\n",
        "from ptx_decompiler.tokenizer import PTXTokenizer, ASTTokenizer\n",
        "\n",
        "df = pd.read_parquet(DATA_PATH)\n",
        "print(f\"Loaded {len(df):,} samples\")\n",
        "\n",
        "ptx_tokenizer = PTXTokenizer(max_vocab_size=2000)\n",
        "print(\"Building PTX vocabulary...\")\n",
        "ptx_tokenizer.build_vocab(df[\"ptx_normalized\"].tolist())\n",
        "print(f\"PTX vocab size: {len(ptx_tokenizer)}\")\n",
        "\n",
        "ast_tokenizer = ASTTokenizer()\n",
        "print(f\"AST vocab size: {len(ast_tokenizer)}\")\n",
        "\n",
        "train_ds, val_ds = load_parquet_for_training(\n",
        "    DATA_PATH, ptx_tokenizer, ast_tokenizer, train_ratio=0.9, seed=42\n",
        ")\n",
        "print(f\"Train: {len(train_ds):,} | Val: {len(val_ds):,}\")\n",
        "\n",
        "curriculum_sampler = CurriculumSampler(train_ds, shuffle=True, seed=42)\n",
        "\n",
        "# With gradient checkpointing, 32 fits tier 3 on T4 16GB; use 16 if OOM\n",
        "BATCH_SIZE = 32 if DEVICE.type == \"cuda\" else 64\n",
        "NUM_WORKERS = 2 if DEVICE.type == \"cuda\" else 0  # parallel data loading on GPU\n",
        "PIN_MEMORY = DEVICE.type == \"cuda\"\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=curriculum_sampler,\n",
        "    collate_fn=lambda b: collate_pad_batch(b, ptx_tokenizer.pad_id, ast_tokenizer.pad_id),\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    persistent_workers=NUM_WORKERS > 0,\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda b: collate_pad_batch(b, ptx_tokenizer.pad_id, ast_tokenizer.pad_id),\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    persistent_workers=NUM_WORKERS > 0,\n",
        ")\n",
        "print(f\"Batch size: {BATCH_SIZE} | Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Model & Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "from ptx_decompiler.model import PTXDecompilerModel\n",
        "from ptx_decompiler.training import Trainer, get_cosine_schedule_with_warmup\n",
        "\n",
        "# Map PTX tokens → AST vocab for copy mechanism\n",
        "ptx_to_ast = torch.full((len(ptx_tokenizer),), -1, dtype=torch.long)\n",
        "for tok, ptx_id in ptx_tokenizer.vocab.items():\n",
        "    if tok in ast_tokenizer.vocab:\n",
        "        ptx_to_ast[ptx_id] = ast_tokenizer.vocab[tok]\n",
        "\n",
        "model = PTXDecompilerModel(\n",
        "    ptx_vocab_size=len(ptx_tokenizer),\n",
        "    ast_vocab_size=len(ast_tokenizer),\n",
        "    d_model=256,\n",
        "    n_heads=8,\n",
        "    d_ff=1024,\n",
        "    encoder_layers=6,\n",
        "    decoder_layers=6,\n",
        "    dropout=0.1,\n",
        "    use_copy=True,\n",
        "    ptx_to_ast_map=ptx_to_ast,\n",
        "    use_gradient_checkpointing=DEVICE.type == \"cuda\",  # save memory so we can use batch 32\n",
        ").to(DEVICE)\n",
        "\n",
        "# torch.compile: faster after first epoch (PyTorch 2.0+, CUDA)\n",
        "USE_COMPILE = True  # set False if first epoch is too slow or errors\n",
        "if USE_COMPILE and DEVICE.type == \"cuda\" and hasattr(torch, \"compile\"):\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "    print(\"Model compiled with torch.compile (first epoch will be slow).\")\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "try:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01, fused=DEVICE.type == \"cuda\")\n",
        "except TypeError:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "num_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=1000, num_training_steps=num_steps\n",
        ")\n",
        "\n",
        "trainer_kw = dict(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=DEVICE,\n",
        "    pad_id_ast=ast_tokenizer.pad_id,\n",
        "    eos_id_ast=ast_tokenizer.eos_id,\n",
        "    label_smoothing=0.1,\n",
        "    use_amp=True,\n",
        "    curriculum_sampler=curriculum_sampler,\n",
        "    save_dir=SAVE_DIR,\n",
        "    use_wandb=False,\n",
        ")\n",
        "if \"val_every\" in inspect.signature(Trainer.__init__).parameters:\n",
        "    trainer_kw[\"val_every\"] = 2  # validate every 2 epochs (faster)\n",
        "trainer = Trainer(**trainer_kw)\n",
        "\n",
        "print(f\"Model parameters: {model.count_parameters():,}\")\n",
        "print(f\"AMP enabled: {trainer.use_amp}\")\n",
        "print(f\"Training on: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train(num_epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final checkpoint\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "final_path = SAVE_DIR / \"checkpoint_final.pt\"\n",
        "torch.save({\"model\": model.state_dict(), \"epoch\": NUM_EPOCHS - 1}, final_path)\n",
        "print(f\"Saved final checkpoint to {final_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
