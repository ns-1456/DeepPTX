{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2â€“3: Train DeepPTX (Colab Pro, A100)\n",
    "\n",
    "Load Parquet dataset, build tokenizers, create DataLoaders with curriculum sampling, and train the Pointer-Generator Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch pyarrow pandas wandb\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"/content/Neural PTX Decompiler\" if os.path.exists(\"/content/Neural PTX Decompiler\") else \"..\"))\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/NeuralPTX/dataset_100k.parquet\"  # or local path\n",
    "SAVE_DIR = Path(\"/content/drive/MyDrive/NeuralPTX/checkpoints\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptx_decompiler.data.dataset import load_parquet_for_training, collate_pad_batch, CurriculumSampler, PTXASTDataset\n",
    "from ptx_decompiler.tokenizer import PTXTokenizer, ASTTokenizer\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "ptx_tokenizer = PTXTokenizer(max_vocab_size=2000)\n",
    "ptx_tokenizer.build_vocab(df[\"ptx_normalized\"].tolist())\n",
    "ast_tokenizer = ASTTokenizer()\n",
    "\n",
    "train_ds, val_ds = load_parquet_for_training(DATA_PATH, ptx_tokenizer, ast_tokenizer, train_ratio=0.9, seed=42)\n",
    "curriculum_sampler = CurriculumSampler(train_ds, shuffle=True, seed=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=128,\n",
    "    sampler=curriculum_sampler,\n",
    "    collate_fn=lambda b: collate_pad_batch(b, ptx_tokenizer.pad_id, ast_tokenizer.pad_id),\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_pad_batch(b, ptx_tokenizer.pad_id, ast_tokenizer.pad_id),\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptx_decompiler.model import PTXDecompilerModel\n",
    "from ptx_decompiler.training import Trainer, get_cosine_schedule_with_warmup\n",
    "\n",
    "ptx_to_ast = torch.full((len(ptx_tokenizer),), -1, dtype=torch.long)\n",
    "for tok, ptx_id in ptx_tokenizer.vocab.items():\n",
    "    if tok in ast_tokenizer.vocab:\n",
    "        ptx_to_ast[ptx_id] = ast_tokenizer.vocab[tok]\n",
    "\n",
    "model = PTXDecompilerModel(\n",
    "    ptx_vocab_size=len(ptx_tokenizer),\n",
    "    ast_vocab_size=len(ast_tokenizer),\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    dropout=0.1,\n",
    "    use_copy=True,\n",
    "    ptx_to_ast_map=ptx_to_ast,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "num_steps = len(train_loader) * 30\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=num_steps)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    pad_id_ast=ast_tokenizer.pad_id,\n",
    "    eos_id_ast=ast_tokenizer.eos_id,\n",
    "    label_smoothing=0.1,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    "    curriculum_sampler=curriculum_sampler,\n",
    "    save_dir=SAVE_DIR,\n",
    "    use_wandb=False,\n",
    ")\n",
    "print(f\"Model parameters: {model.count_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
