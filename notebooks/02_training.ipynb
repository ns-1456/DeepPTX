{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2–3: Train DeepPTX\n",
    "\n",
    "Load Parquet dataset, build tokenizers, create DataLoaders with curriculum sampling,\n",
    "and train the Pointer-Generator Transformer.\n",
    "\n",
    "**Works on:**\n",
    "- Google Colab (T4/A100) — fastest, uses CUDA AMP\n",
    "- Mac M1/M2/M3 — uses MPS acceleration (no AMP, still fast)\n",
    "- CPU — slowest fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run this cell first on Google Colab to clone the repo ---\n",
    "import os\n",
    "if os.path.exists(\"/content\") and not os.path.exists(\"/content/DeepPTX\"):\n",
    "    !git clone https://github.com/ns-1456/DeepPTX.git /content/DeepPTX\n",
    "    %cd /content/DeepPTX\n",
    "    !pip install -q torch pyarrow pandas wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_ROOT = \"/content/DeepPTX\"\n",
    "else:\n",
    "    REPO_ROOT = os.path.abspath(\"..\")\n",
    "\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ptx_decompiler.utils import get_device, supports_amp, is_colab\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"AMP supported: {supports_amp(DEVICE)}\")\n",
    "if DEVICE.type == \"mps\":\n",
    "    print(\"Apple Silicon detected — using MPS acceleration\")\n",
    "    print(\"Tip: MPS is ~3-5x faster than CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Paths =======================\n",
    "# Colab: data on Drive or in /content\n",
    "# Local Mac: data in repo root (generated by notebook 01)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Option 1: Google Drive (persistent)\n",
    "    # from google.colab import drive; drive.mount(\"/content/drive\")\n",
    "    # DATA_PATH = \"/content/drive/MyDrive/NeuralPTX/dataset_100k.parquet\"\n",
    "    # SAVE_DIR  = Path(\"/content/drive/MyDrive/NeuralPTX/checkpoints\")\n",
    "\n",
    "    # Option 2: Local to Colab (faster I/O, lost on disconnect)\n",
    "    DATA_PATH = os.path.join(REPO_ROOT, \"dataset_100k.parquet\")\n",
    "    SAVE_DIR  = Path(REPO_ROOT) / \"checkpoints\"\n",
    "else:\n",
    "    # Local Mac / Linux\n",
    "    DATA_PATH = os.path.join(REPO_ROOT, \"dataset_100k.parquet\")\n",
    "    SAVE_DIR  = Path(REPO_ROOT) / \"checkpoints\"\n",
    "\n",
    "print(f\"Data: {DATA_PATH}\")\n",
    "print(f\"Checkpoints: {SAVE_DIR}\")\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH} — run notebook 01 first!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Build Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptx_decompiler.data.dataset import load_parquet_for_training, collate_pad_batch, CurriculumSampler, PTXASTDataset\n",
    "from ptx_decompiler.tokenizer import PTXTokenizer, ASTTokenizer\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded {len(df):,} samples\")\n",
    "\n",
    "ptx_tokenizer = PTXTokenizer(max_vocab_size=2000)\n",
    "print(\"Building PTX vocabulary...\")\n",
    "ptx_tokenizer.build_vocab(df[\"ptx_normalized\"].tolist())\n",
    "print(f\"PTX vocab size: {len(ptx_tokenizer)}\")\n",
    "\n",
    "ast_tokenizer = ASTTokenizer()\n",
    "print(f\"AST vocab size: {len(ast_tokenizer)}\")\n",
    "\n",
    "train_ds, val_ds = load_parquet_for_training(\n",
    "    DATA_PATH, ptx_tokenizer, ast_tokenizer, train_ratio=0.9, seed=42\n",
    ")\n",
    "print(f\"Train: {len(train_ds):,} | Val: {len(val_ds):,}\")\n",
    "\n",
    "curriculum_sampler = CurriculumSampler(train_ds, shuffle=True, seed=42)\n",
    "\n",
    "# Batch size: smaller on MPS/CPU to avoid OOM\n",
    "BATCH_SIZE = 128 if DEVICE.type == \"cuda\" else 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=curriculum_sampler,\n",
    "    collate_fn=lambda b: collate_pad_batch(b, ptx_tokenizer.pad_id, ast_tokenizer.pad_id),\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_pad_batch(b, ptx_tokenizer.pad_id, ast_tokenizer.pad_id),\n",
    "    num_workers=0,\n",
    ")\n",
    "print(f\"Batch size: {BATCH_SIZE} | Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptx_decompiler.model import PTXDecompilerModel\n",
    "from ptx_decompiler.training import Trainer, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Map PTX tokens → AST vocab for copy mechanism\n",
    "ptx_to_ast = torch.full((len(ptx_tokenizer),), -1, dtype=torch.long)\n",
    "for tok, ptx_id in ptx_tokenizer.vocab.items():\n",
    "    if tok in ast_tokenizer.vocab:\n",
    "        ptx_to_ast[ptx_id] = ast_tokenizer.vocab[tok]\n",
    "\n",
    "model = PTXDecompilerModel(\n",
    "    ptx_vocab_size=len(ptx_tokenizer),\n",
    "    ast_vocab_size=len(ast_tokenizer),\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    dropout=0.1,\n",
    "    use_copy=True,\n",
    "    ptx_to_ast_map=ptx_to_ast,\n",
    ").to(DEVICE)\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "num_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=1000, num_training_steps=num_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    pad_id_ast=ast_tokenizer.pad_id,\n",
    "    eos_id_ast=ast_tokenizer.eos_id,\n",
    "    label_smoothing=0.1,\n",
    "    use_amp=True,                  # auto-disabled on MPS/CPU by Trainer\n",
    "    curriculum_sampler=curriculum_sampler,\n",
    "    save_dir=SAVE_DIR,\n",
    "    use_wandb=False,\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"AMP enabled: {trainer.use_amp}\")\n",
    "print(f\"Training on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "final_path = SAVE_DIR / \"checkpoint_final.pt\"\n",
    "torch.save({\"model\": model.state_dict(), \"epoch\": NUM_EPOCHS - 1}, final_path)\n",
    "print(f\"Saved final checkpoint to {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
